## Workshop Info and Materials


### 1. Machine Learning Model Validation for Critical or Regulated Applications

June 29 - July 6, 2022 | Online: QU-ML Model Validation Workshop

URL: [https://github.com/SelfExplainML/PiML-Toolbox](https://github.com/SelfExplainML/PiML-Toolbox)

Machine Learning (ML) has gained significant adoption in the industry; however, many concerns remained for their usage in highly regulated or critical applications. As with any models, understanding and testing the risk of ML are front and center in the discipline of model risk management. Given the data driven approach as well as the complexity of ML algorithms, we need to improve the sophistication of model design and validation to evaluate both their conceptual soundness as well as outcome. Key element for the conceptual soundness evaluation is model explainability/interpretability. Comprehensive Machine Learning model validation for real production require analysis beyond the standard model performance evaluation which must cover: identification of model weakness through residual slicing, identification of overfitting and underfitting regions, model robustness under noisy or corrupted inputs, prediction reliability such as evaluation of prediction uncertainty and resilience of model performance under input distribution drift. PiML (Python Interpretable Machine Learning) was created to address all the aforementioned needs in a single easy to use (low code) packages.

We are going to cover the above aspects — including hands on experience using PiML — in a two-session seminar:

**Session 1: Machine Learning Interpretability**

[Slides](#) and [Jupyter Notebooks](#) will be available.

- Post-hoc explainability tools for black box models
  - Local explainability: LIME and SHAP
  - Global explainability: Variable Importance (VI), PDP and ALE
- Limitation and pitfalls of post hoc explainability
- Deep ReLU Networks as Inherehently Interpretable Models
  - Local Linear Model Representation of ReLU DNN and Interpretability
  - Controlling Model Complexity through Regularization
- Functional ANOVA (FANOVA) and Interpretable Model Representation
  - Explainable Boosting Machine
  - Generalized Additive Model with structed Interactions (GAMI) Networks

**Session 2: Machine Learning Model Diagnostics and Validation**

[Slides](#) and [Jupyter Notebooks](#) will be available.

- Model Diagnostics and Testing
  - Weak Spot Analysis through Error Slicing
  - Identification of Over and Underfitting Regions
  - Robustness Testing
  - Reliability (Prediction Uncertainty) Testing using Conformal Prediction
  - Resiliency Testing under Input Distribution Drift
- Model Comparison
  - Arbitrary black box vs. Inherently interpretable models
  - Performance, Robustness and Resilience

### Machine Learning Model Validation Masterclass 

May 9, 2022 | New York City: [RISK AMERICAS 2022 Model Validation Workshop](https://www.cefpro.com/forthcoming-events/risk-americas/#section-1643126595388-11-2)

The focus of this workshop is to provide comprehensive approach for machine learning model validation with special emphasis on conceptual soundness and outcome analysis. The key elements includes: model explainability, model weakness identification, prediction reliability, model robustness under changing environment and fairness. The workshop will spend significant amount of time on inherently interpretable models due to their key role for high risk applications as well as model benchmarks. This is a hands on workshop where the participants will learn practical concepts along with exercise using Python in Google Colab. Low code python packages will be provided so that participants with minimum familiarity of Python will be able to follow without difficulty.
