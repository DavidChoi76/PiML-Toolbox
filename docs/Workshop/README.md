## Workshop Info and Materials

<details open>
  <summary><strong> Machine Learning Model Validation MasterClass, May 9, 2022 | New York City</strong></summary><br />

**URL:** [RISK AMERICAS 2022 Model Validation Workshop](https://www.cefpro.com/forthcoming-events/risk-americas/#section-1643126595388-11-2)

**Speakers:** Agus Sudjianto and Vijay Nair

**Slides:** 
  - [Part 1: Introduction](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part1%20Intro.pdf)
  - [Part 2: Machine Learning Algorithms and Explainability](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part2%20Ml-Algorithms-Explainability.pdf)
  - [Part 3: Unwrapping ReLU Deep Neural Networks](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part3%20ReLU-DNN.pdf)
  - [Part 4: Inherently Interpretable Models - EBM and GAMI-Net](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part4%20EBM%20and%20GAMI-Net.pdf)
  - [Part 5: Outcome Testing](https://github.com/SelfExplainML/PiML-Toolbox/blob/main/docs/Workshop/202205MasterClass-Part5%20Outcome%20Testing.pdf)

The focus of this workshop is to provide comprehensive approach for machine learning model validation with special emphasis on conceptual soundness and outcome analysis. The key elements includes: model explainability, model weakness identification, prediction reliability, model robustness under changing environment and fairness. The workshop will spend significant amount of time on inherently interpretable models due to their key role for high risk applications as well as model benchmarks. This is a hands on workshop where the participants will learn practical concepts along with exercise using Python in Google Colab. Low code python packages will be provided so that participants with minimum familiarity of Python will be able to follow without difficulty.
  
**Session 1: Introduction and machine learning explainability**

- Elements of machine learning validation: Conceptual soundness and outcome analysis
- Introduction to key concepts: explainability, robustness, reliability and fairness
- Post-hoc explainability tools
  - Local explainability: LIME and SHAP 
  - Global explainability: Variable Importance,  Partial Dependence and Accumulated Local Effects

**Session 2: Designing inherently interpretable model**

- Limitation of post-hoc explainability
- Introduction to building inherently interpretable model
  - Explainable boosting machine
  - GAMI Neural Networks

**Session 3: Deep ReLU networks as interpretable models**

- Local partition and linear models
- Model interpretation and diagnostics
- Complexity control through regularization

**Session 4: Outcome testing**

- Identification of performance weakness through slicing
- Reliability evaluation through conformal prediction
- Robustness evaluation for covariate/distribution drift
- Fairness testing

</details>  

<details open>
  <summary><strong>Machine Learning Model Validation for Critical or Regulated Applications, June 29 - July 6, 2022 | Online</strong></summary><br /> 

**URL:** [QU-ML Model Validation Workshop](https://mlmodelvalidation.splashthat.com/)

**Speakers:** Agus Sudjianto and Aijun Zhang

**Slides:** to be shared.

**Codes:** to be shared.
  
Machine Learning (ML) has gained significant adoption in the industry; however, many concerns remained for their usage in highly regulated or critical applications. As with any models, understanding and testing the risk of ML are front and center in the discipline of model risk management. Given the data driven approach as well as the complexity of ML algorithms, we need to improve the sophistication of model design and validation to evaluate both their conceptual soundness as well as outcome. Key element for the conceptual soundness evaluation is model explainability/interpretability. Comprehensive Machine Learning model validation for real production require analysis beyond the standard model performance evaluation which must cover: identification of model weakness through residual slicing, identification of overfitting and underfitting regions, model robustness under noisy or corrupted inputs, prediction reliability such as evaluation of prediction uncertainty and resilience of model performance under input distribution drift. PiML (Python Interpretable Machine Learning) was created to address all the aforementioned needs in a single easy to use (low code) packages.

We are going to cover the above aspects — including hands on experience using PiML — in a two-session seminar:

**Session 1: Machine Learning Interpretability**

- Post-hoc explainability tools for black box models
  - Local explainability: LIME and SHAP
  - Global explainability: Variable Importance (VI), PDP and ALE
- Limitation and pitfalls of post hoc explainability
- Deep ReLU Networks as Inherehently Interpretable Models
  - Local Linear Model Representation of ReLU DNN and Interpretability
  - Controlling Model Complexity through Regularization
- Functional ANOVA (FANOVA) and Interpretable Model Representation
  - Explainable Boosting Machine
  - Generalized Additive Model with structed Interactions (GAMI) Networks

**Session 2: Machine Learning Model Diagnostics and Validation**

- Model Diagnostics and Testing
  - Weak Spot Analysis through Error Slicing
  - Identification of Over and Underfitting Regions
  - Robustness Testing
  - Reliability (Prediction Uncertainty) Testing using Conformal Prediction
  - Resiliency Testing under Input Distribution Drift
- Model Comparison
  - Arbitrary black box vs. Inherently interpretable models
  - Performance, Robustness and Resilience

</details>  

